\documentclass[aspectratio=169]{../latex_main/tntbeamer}  % you can pass all options of the beamer class, e.g., 'handout' or 'aspectratio=43'
\input{../latex_main/preamble}

\title[Statistics]{DS: Bias and Variance}
\subtitle{Complexity}

\graphicspath{ {./figure/} }
%\institute{}


\begin{document}
	
	\maketitle
	\begin{frame}[c]{Modeling Goals}
	    \begin{itemize}
	        \item Try to minimize all three of observation variance, model bias, and model variance.
	    \end{itemize}
	    But
	    \begin{itemize}
	        \item Observation variance is often out of our control
	        \item Reducing complexity to reduce model variance can increase bias
	        \item Increasing model complexity to reduce bias can increase model variance
	        \item We can estimate the overall empirical risk, but
	        \begin{itemize}
	           \item estimating the concrete trade-off between bias and variance can often only be costly approximated
	           \item Most important that you know about the general trade-off
	        \end{itemize}
	        \item \alert{Domain knowledge matters:} the right model structure!

	        
	    \end{itemize}
	\end{frame}
	
	
	\begin{frame}{Bias Variance Plot}
	    \centering
	    \includegraphics[scale=.4]{Bild14}
	\end{frame}
	
	\begin{frame}{The right model structure matters!}
	    \begin{columns}
	        \begin{column}{.5\textwidth}
	                \begin{figure}
	                    \includegraphics[scale=.35]{Bild15}
	                \end{figure}
	                Ptolemaic Astronomy, a geocentric model based on circular orbits (epicycles and deferents).\\
	                \bigskip
	                High accuracy but very high model complexity.
	        \end{column}
	        
	        
	        \begin{column}{.5\textwidth}
	                \begin{figure}
	                    \includegraphics[scale=.35]{Bild16}
	                \end{figure}
	                Copernicus and Kepler: a heliocentric model with elliptical orbits.\\
	                \bigskip
	                Small model complexity yet high accuracy.

	        \end{column}
	    \end{columns}
	\end{frame}
	
	\begin{frame}{Bias Variance Plot}
	\vspace{-2em}
	   \begin{center}
	        \includegraphics[scale=.2]{Bild14}   
	   \end{center}
	    
	 What can we learn from this trade-off?
	 
	 \begin{itemize}
	     \item If error increases with higher model complexity, then too high variance $\leadsto$ reduce model complexity!
	     \item If error increases with smaller model complexity, then too large bias $\leadsto$ increase model complexity!
	 \end{itemize}
	 
	 Correct? No!
	 $\leadsto$ Counter example: Deep Learning, i.e., very high model complexity and still low test error
	    
	\end{frame}
	
	\begin{frame}{Conclusion from Bias-Variance Trade-off?}
	\vspace{-2em}
	   \begin{center}
	        \includegraphics[scale=.2]{Bild14}   
	   \end{center}
	    
	 What can we learn from this trade-off?
	 
	 \begin{itemize}
	     \item If error increases with higher model complexity, then too high variance $\leadsto$ reduce model complexity!
	     \item If error increases with smaller model complexity, then too large bias $\leadsto$ increase model complexity!
	 \end{itemize}
	 
	 Correct? Not Always!\\
	 $\leadsto$ Counter example: Deep Learning, i.e., very high model complexity and still low test error
	    
	\end{frame}
	
	\begin{frame}{Double Descent Phenomenon}
	\vspace{-2em}
	   \begin{center}
	        \includegraphics[scale=.2]{08_bias_and_variance/figure/double_descent.PNG}   
	   \end{center}
    
    \begin{itemize}
        \item DNNs are very over-parameterized compared to the underlying data complexity
        \item You can still train DNNs effectively
        \item Current theory called ``Double Descent Phenomenon''
        \begin{itemize}
            \item For some model classes (not all!), increasing model complexity (even beyond reaching training error of 0) can improve generalization error
        \end{itemize}
    \end{itemize}    

	\end{frame}
	
	
\end{document}