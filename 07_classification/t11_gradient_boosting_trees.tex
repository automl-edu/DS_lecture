\documentclass[aspectratio=169]{../latex_main/tntbeamer}  % you can pass all options of the beamer class, e.g., 'handout' or 'aspectratio=43'
\input{../latex_main/preamble_2}

\title[Gradient-Boosting Trees]{DS: Decision Trees}
\subtitle{Gradient-Boosting Trees}

\graphicspath{ {./figure_tree/} }
%\institute{}


\begin{document}
	
	\maketitle
	\begin{frame}[c]{Gradient-Boosting Trees}

    \begin{itemize}
        \item The most popular model in Kaggle Competitions\footnote{\url{https://www.kaggle.com/}} for tabular data is XGBoost
        \begin{itemize}
            \item XGBoost = Extreme Gradient Boosted Trees
        \end{itemize}
        \smallskip
        \item Main Idea:
        \begin{itemize}
            \item Random Forest build an ensemble of independent decision trees
            \item Instead of independent trees, try to improve the first trees with the following trees
            \item Each additional tree should directly contribute to improve the ensemble (not only by chance!)
        \end{itemize}
    \end{itemize}

	\end{frame}
	
	\begin{frame}[c]{Correcting Predictions}

    \vspace{-1em}
    \begin{itemize}
        \item Let's assume that we fitted $m$ trees $\hat{F}_m$ already
        \item All of these together provide a prediction for a given input:
        $$ \hat{F}_m(\bm{x}_i) \to \hat{y}_i $$
        \smallskip
        \item The model might be off from the true label $y_i$ to a certain amount
        $$e_i = y_i - \hat{F}_m(\bm{x}_i)$$
        \item $e_i$ is called a residual.
        \smallskip
        \item Can we add a model that correct our ensemble $\hat{F}_m$ in the directions of the residuals by adding another model $\hat{f}_{m+1}$?
        $$ \hat{f}_{m+1} :=  y_i - \hat{F}_m(\bm{x}_i)$$ 
        \item if $ \hat{f}_{m+1}$ could perfectly learn the residuals, $\hat{f}_{m+1} +  \hat{F}_m(\bm{x}_i)$ would be a perfect model
    \end{itemize}

	\end{frame}
	
	\begin{frame}[c]{Learning how to boost}

    \vspace{-1em}
    \begin{itemize}
        \item Coming back to the idea of ensembles $F$ (with $M$ base models) by weighting $\gamma$ each model:
        
        $$ \hat{F}(\bm{x}) = \sum_{m=1}^{M} \gamma_m \hat{f}_m(\bm{x}) + c $$
        \item Let's start with fitting a first constant base model $\hat{F}_0$ (with constant prediction $\gamma_0$) on all training data
        $$\hat{F}_0 \in \argmin_{\gamma_0} \sum \mathcal{L}(y_i, \gamma_0) $$
        \item Extend the ensemble by a base model correcting for the residuals -- recursive definition!
        
        $$ \hat{F}_m (\bm{x}) = \hat{F}_{m-1}(\bm{x}) + \argmin_{\hat{f}_m} \left[ \gamma_m \sum_i \mathcal{L}(y_i, \hat{F}_{m-1}(\bm{x}_i) + \hat{f}_m(\bm{x}_i)) \right] $$
    \end{itemize}

	\end{frame}
	
	\begin{frame}[c]{Learning how to boost (cont'd)}

    \vspace{-1em}
    $$ \hat{F}_m (\bm{x}) = \hat{F}_{m-1}(\bm{x}) + \argmin_{\hat{f}_m} \left[ \gamma_m \sum_i \mathcal{L}(y_i, \hat{F}_{m-1}(\bm{x}_i) + \hat{f}_m(\bm{x}_i)) \right] $$
    \begin{itemize}
        \item[$\leadsto$] that would be an infeasible optimization problem
        \item[$\leadsto$] take a step in the steepest direction (steepest descent) of this minimization problem:
        $$ \hat{F}_m(x) = \hat{F}_{m-1}(\bm{x}) - \gamma_m \sum_i \nabla_{\hat{F}_{m-1}} \mathcal{L}(y_i, \hat{F}_{m-1}(\bm{x_i})) $$
        \item where $\gamma_m$ is the step length (a.k.a learning rate)
    \end{itemize}

	\end{frame}
	
	\begin{frame}[c]{Practical Considerations}

    \begin{itemize}
        \item Base models should be very shallow models, e.g., decisions trees with a single split
        \smallskip
        \item Learning rate $\gamma_m$ decides how many trees you need to change the predictions
        \begin{itemize}
            \item too large $\gamma_m$ will lead to unstable learning
            \item too small $\gamma_m$ will lead to very slow learning and large models
            \item[$\leadsto$] often between $0.3$ and $0.1$ $\leadsto$ better to tune it explicitly (e.g., with Bayesian Optimization)
        \end{itemize}
        \smallskip
        \item One of the most robust and well-performing ML models for tabular data
        \begin{itemize}
            \item Nevertheless, good feature engineering and preprocessing can be crucial
        \end{itemize}
        \smallskip
        \item XGBoost is one of the most well-known libraries for gradient-boosting trees:\\ \url{https://xgboost.readthedocs.io/en/stable/index.html}
    \end{itemize}

	\end{frame}
	
	
\end{document}