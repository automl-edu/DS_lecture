\documentclass[aspectratio=169]{../latex_main/tntbeamer}  % you can pass all options of the beamer class, e.g., 'handout' or 'aspectratio=43'
\input{../latex_main/preamble_2}

\title[DL: Neural Architectures]{DS: Deep Learning}
\subtitle{Neural Architectures}

\date{\hspace{0.5em} {\includegraphics[height=1.5em]{../latex_main/figures/Cc-by-nc-sa_icon.svg.png}}}

\graphicspath{ {./figure/} }
%\institute{}


\begin{document}
	
	\maketitle
    	\begin{frame}{DNN Architectures}

            \begin{itemize}
                \item Neural networks have various architectures designed for specific tasks.
                \item The use of traditional MLP architectures is very rarely seen these days.
                \item Understanding different architectures is crucial for applying them effectively in real-world applications.
                \medskip
                \item In the following, let's talk about one of the most famous architecture types
            \end{itemize}
    
                    
    	\end{frame}

        %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
        %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
        \begin{frame}{Convolutional Neural Networks (CNNs)}
        \begin{itemize}
            \item Designed primarily for processing data that comes in the form of multiple arrays, such as images.
            \item Utilizes convolutional layers, pooling layers, and fully connected layers.
            \item Excel at capturing spatial hierarchies in data.
        \end{itemize}
        \centering
        \includegraphics[width=0.7\textwidth]{figure/conv.png}\\
        Credits: Andreas Maier, CC BY 4.0
        \end{frame}

        %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
        %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
        \begin{frame}
        \frametitle{Residual Networks (ResNet)}
        \begin{itemize}
            \item Introduced to address the vanishing gradients problem in very deep networks by using skip connections.
            \item Allows training of much deeper networks by enabling feature reuse and easier gradient flow.
            \item Commonly used in tasks requiring very deep networks, such as image classification and recognition.
        \end{itemize}
        \centering
        \includegraphics[width=0.4\textwidth]{figure/resnet.png}\\
        Credits: [\href{https://arxiv.org/pdf/1512.03385}{He et al. 2015}]
        \end{frame}

        %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
        %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
        \begin{frame}
        \frametitle{Auto-Encoders}
        \begin{itemize}
            \item An unsupervised learning technique used to learn efficient codings of unlabeled data.
            \item Consists of an encoder and a decoder. The encoder compresses the input and the decoder reconstructs the output from this representation.
            \item Used for tasks like dimensionality reduction, feature learning, and more.
        \end{itemize}
        \centering
        \includegraphics[width=0.5\textwidth]{figure/ae.png}
        \end{frame}

        %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
        %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
        \begin{frame}
        \frametitle{GAN: Generative Adversarial Networks}
        \begin{itemize}
            \item Fairly easy to fool DNNs by providing a slightly different image that leads to a different prediction, but a human it is not able to tell the difference between both images $\leadsto$ adversarial attack
            \item GANs are trained to mitigate the chance of adversarial attacks and to increase robustness 
            \item The generator generates images similar to the training dataset to fool the DNN. The discriminator has to distinguish between real and fake images
        \end{itemize}
        \centering
        \includegraphics[width=0.5\textwidth]{figure/gans.png}\\
        Credits: [\href{https://sthalles.github.io/intro-to-gans/}{Silva 2017}]
        \end{frame}

        %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
        %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
        \begin{frame}{Recurrent Neural Networks (RNNs)}
        \begin{itemize}
            \item Suited for sequential data such as time series or natural language.
            \item Has the ability to retain information in 'memory' over time through loops.
            \item Challenges include difficulties with long-range dependencies, see LSTM or GRU.
        \end{itemize}
        \centering
        \includegraphics[width=0.3\textwidth]{figure/rnn.png}
        \end{frame}

        %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
        %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
        \begin{frame}{Long Short-Term Memory Networks (LSTMs)}
        \begin{itemize}
            \item A type of RNN designed to remember long-term dependencies.
            \item LSTMs are particularly useful in tasks where context from the distant past is informative.
            \item Overcomes the vanishing gradient problem typical of standard RNNs.
        \end{itemize}
        \centering
        \includegraphics[width=0.5\textwidth]{figure/lstm.png}\\
        Credits: Cosmia Nebula, CC BY-SA 4.0
        \end{frame}

        %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


        %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
        \begin{frame}{Tranformers}

        \begin{columns}

        \begin{column}{0.5\textwidth}

            \begin{itemize}
                \item State of the art for text data, but also fairly prominent in computer vision (as an alternative to CNNs)
                \item Attention mechanism allows to learn to focus on context information; i.e., not only the most recent word in a sentence is important to predict the next, but other previous keywords, too
            \end{itemize}

        \end{column}

        \begin{column}{0.5\textwidth}

        \centering
        \vspace*{-2em}
        \includegraphics[width=0.6\textwidth]{figure/transformer.png}\\
        Credits: [\href{https://arxiv.org/pdf/1706.03762}{Vaswani et al. 2017}]

        \end{column}
            
        \end{columns}
        


        \end{frame}

        %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
        \begin{frame}{Conclusion}


            \begin{itemize}
                \item There are a whole bunch of different architectures
                \begin{itemize}
                    \item Even more than we discussed here
                \end{itemize}
                \item Depending on your data modality, your modelling assumptions and overall task, you have to choose one of them 
                \item Furthermore, you have to decide the size of architectures (e.g., width, depth, channels) and certain operators
                \begin{itemize}
                    \item[$\leadsto$] Can be partially automated with \emph{neural architecture search} [\href{https://arxiv.org/abs/1808.05377}{Elsken et al. 2018}]
                \end{itemize}
            \end{itemize}


        


        \end{frame}

        %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
 	
\end{document}