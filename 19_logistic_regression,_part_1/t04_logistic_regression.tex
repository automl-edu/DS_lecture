\documentclass[aspectratio=169]{../latex_main/tntbeamer}  % you can pass all options of the beamer class, e.g., 'handout' or 'aspectratio=43'
\input{../latex_main/preamble}

\title[Introduction]{DS: Logistic Regression, Part 1}
\subtitle{Logistic regression}

\graphicspath{ {./figure/} }
%\institute{}


\begin{document}
	
	\maketitle
	\begin{frame}{Linear vs. logistic regression}
	    In a linear regression model, we predict a quantitative variable (i.e., some real number) as a linear function of features.
	    \begin{itemize}
	        \item Our output, or response, y, could be any real number.
	    \end{itemize}
	    \begin{equation*}
	        \hat{y} = f_\theta (x) = x^T\theta
	    \end{equation*}
	    
	    In a logistic regression model, our goal is to predict a binary categorical variable (class 0 or class 1) as a linear function of features, passed through the logistic function.
	    \begin{itemize}
	        \item Our response is the probability that our observation belongs to class 1.
	        \item Haven’t yet done classification!
	    \end{itemize}
	    \begin{equation*}
	        \hat{y} =f_\theta (x) = P(Y=1|x) = \sigma (x^T\theta)
	    \end{equation*}
	\end{frame}
	
	
	\begin{frame}{Example calculation}
	   
	   \begin{columns}
	    
	   \begin{column}{.5\textwidth}
	    Suppose I want to predict the probability that LeBron’s shot goes in, given shot distance (first feature) and \# of seconds left on the shot clock (second feature).\\
	    \bigskip
	    I fit a logistic regression model using my training data, and somehow compute
	    \begin{equation*}
	        \hat{y}^T = [0.1 , -0.5]
	    \end{equation*}
	    
	   Under the logistic model, compute the probability his shot goes in, given that
	    \begin{itemize}
	        \item He shoots it from 15 feet.
	        \item There is 1 second left on the shot clock.
	    \end{itemize}
	    \end{column}
	    \begin{column}{.5\textwidth}
	        \begin{figure}
	            \centering
	            \includegraphics[scale=.9]{Bild7}
	        \end{figure}
	    \end{column}
	    
	    \end{columns}
	\end{frame}
	
	
	\begin{frame}{Example calculation (solution)}
	   
	   \begin{columns}
	    
	   \begin{column}{.5\textwidth}
	   \begin{equation*}
	       x^T = [15, 1]\hspace{2cm} \hat{\theta}^T = [0.1, -0.5]
	   \end{equation*}
	   
	   \begin{align*}
	       P(Y=1|x) &= \sigma (x^T\hat{\theta})\\
	       &= \sigma (\hat{\theta}_1 \cdot \text{SHOT DISTANCE} + \hat{\theta}_2 \cdot \text{SECONDS LEFT} )\\
	       &=  \sigma (0.1 \cdot 15 + (-0.5) \cdot 1 )\\
	       &= \sigma (1)\\
	       &= \frac{1}{1 + e^{-1}}\\
	       &\approx 0.7311
	   \end{align*}
	    \end{column}
	    \begin{column}{.5\textwidth}
	        \begin{figure}
	            \centering
	            \includegraphics[scale=.5]{Bild8}
	        \end{figure}
	    \end{column}
	    
	    \end{columns}
	\end{frame}
	
	
	\begin{frame}{Properties of the logistic function}
	   
	   \begin{columns}
	    
	   \begin{column}{.5\textwidth}
	   The logistic function is a type of sigmoid, a class of functions that share certain properties.
	   \begin{align*}
	       \sigma (t) = \frac{1}{1 + e^-t} \hspace{1cm} -\infty < t < \infty
	   \end{align*}
	   \begin{itemize}
	       \item Its output is bounded between 0 and 1, no matter how large t is.
	       \begin{itemize}
	           \item Fixes an issue with using linear regression to predict probabilities.
	       \end{itemize}
	       \item We can interpret it as mapping real numbers to probabilities.
	   \end{itemize}
	    \end{column}
	    \begin{column}{.5\textwidth}
	        \begin{figure}
	            \centering
	            \includegraphics[scale=.65]{Bild9}
	        \end{figure}
	    \end{column}
	    
	    \end{columns}
	\end{frame}
	
	
	\begin{frame}{Properties of the logistic function}
	   %\begin{align*}
	   %    &\text{Definition}\\
	   %    &\sigma (t) = \frac{1}{1 + e{-t}} = \frac{e^t}{1+e^t} \hspace{2cm}\\
	   %    &\text{Range}\\
	   %    &0 < \sigma (t) < 1\\
	   %    &\text{Inverse}\\
	   %    & t = \sigma^{-1}(p) = log\left(\frac{p}{1-p}\right)\\
	   %    &\text{Reflection and Symmetry}\\
	   %    &1 - \sigma (t) = \frac{e^{-t}}{1 + e^{-t}} = \sigma (-t)\\
	   %    &\text{Derivative}\\
	   %    &\frac{d}{dt}\sigma \sigma (t) = \sigma (t)(1 - \sigma (t)) = \sigma (t)\sigma (-t)
	   %\end{align*}
	   \centering
        \includegraphics[scale=.7]{Bild11}
	\end{frame}
	
	
	
	\begin{frame}{Shape of the logistic function}
	   
	   \begin{columns}
	    
	   \begin{column}{.5\textwidth}
	   Consider the plot of       $\sigma (\theta_1x)$      , for several different values of $\theta_1$. 
	   \begin{itemize}
	       \item If $\theta_1$ is positive, the curve increases to the right.
	       \item The further  $\theta_1$ is from 0, the steeper the curve.
	   \end{itemize}
	   In the notebook, we explore more sophisticated logistic curves.
	    \end{column}
	    \begin{column}{.5\textwidth}
	        \begin{figure}
	            \centering
	            \includegraphics[scale=.3]{Bild10}
	        \end{figure}
	    \end{column}
	    
	    \end{columns}
	\end{frame}
	
	
	\begin{frame}{Parameter interpretation}
	   
	   Recall, we arrived at the model by assuming that the log-odds of the probability of belonging to class 1 was linear.
	   \begin{equation*}
	       P(Y=1|x) = \sigma(x^T\theta) \leftarrow log\left(\frac{P(Y=1|x)}{P(Y=0|x)}\right) = x^T\theta \leftarrow \frac{P(Y=1|x)}{P(Y=0|x)} = e^{x^T\theta}
	   \end{equation*}
	   $\frac{P(Y=1|x)}{P(Y=0|x)}$: This is the same as     $\frac{p}{1-p}$          , because
        \begin{equation*}
            P(Y=1|x) + P(Y=0|x) = 1
        \end{equation*}
        (Remember, we are dealing with binary classification – we are predicting 1 or 0.)
	\end{frame}
	
	
	\begin{frame}{Parameter interpretation}
	  Let’s suppose our linear component has just a single feature, along with an intercept term.
	   \begin{equation*}
	       \frac{P(Y=1|x)}{P(Y=0|x)} = e^{\theta_0 + \theta_1x}
	   \end{equation*}
	  What happens if you increase x by one unit?
	  \begin{itemize}
	      \item Odds is multiplied by $e^{\theta_1}$
	      \item If      $\theta_1$ > 0         , the odds increase.
	      \item If         $\theta_1$ < 0      , the odds increase.
	  \end{itemize}
	  What happens if              $x^T\theta = \theta_0 + \theta_1x = 0$?
	  \begin{itemize}
	      \item This means class 1 and class 0 are equally likely.
	      \item $e^0 = 1 \rightarrow \frac{P(Y=1|x)}{P(Y=0|x)} = 1 \rightarrow P(Y=1|x) = P(Y=0|x)$
	  \end{itemize}
	  The odds ratio can be interpreted as the “number of successes for each failure.”
	\end{frame}
\end{document}