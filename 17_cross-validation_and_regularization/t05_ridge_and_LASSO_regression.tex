\documentclass[aspectratio=169]{../latex_main/tntbeamer}  % you can pass all options of the beamer class, e.g., 'handout' or 'aspectratio=43'
\input{../latex_main/preamble}

\title[Introduction]{DS: Cross-Validation and Regularization}
\subtitle{Ridge and LASSO Regression}

\graphicspath{ {./figure/} }
%\institute{}


\begin{document}
	
	\maketitle
	\begin{frame}{Ridge Regression}
	    “Ridge Regression” is a term for the following specific combination of model, loss, and regularization:
	    \begin{itemize}
	        \item Model: $\mathbb{\hat{Y}} = \mathbb{X}\hat{\theta}$
	        \item Loss: Squared loss
	        \item Regularization: L2 regularization
	    \end{itemize}
	    The objective function we minimize for Ridge Regression is average squared loss, plus an added penalty:
	    \begin{equation*}
	        \hat{\theta}_{ridge} = \underset{\theta}{\text{arg min}}\frac{1}{n}||\mathbb{Y} - \mathbb{X}\theta||_2^2 + \lambda \sum\limits_{j=1}^d\theta_i^2 
	    \end{equation*}
	\end{frame}
	
	
	\begin{frame}{Ridge Regression}
	    We can also express this objective slightly differently:
	    \begin{equation*}
	        \hat{\theta}_{ridge} = \underset{\theta}{\text{arg min}}\frac{1}{n}||\mathbb{Y} - \mathbb{X}\theta||_2^2 + \lambda \sum\limits_{j=1}^d\theta_i^2 
	    \end{equation*}
	    
	    \begin{equation*}
	        \hat{\theta}_{ridge} = \underset{\theta}{\text{arg min}}\frac{1}{n}||\mathbb{Y} - \mathbb{X}\theta||_2^2 + \lambda ||\theta ||_2^2
	    \end{equation*}
	    $||\theta ||$: L2 norm of $\theta$ (hence, L2 regularization)\\
	    \bigskip
	    The latter representation ignores the fact that we typically don’t regularize the intercept term
	\end{frame}
	
	
	\begin{frame}[c]{Ridge Regression}
	    Ridge Regression has a closed form solution, conveniently:
	    \begin{equation*}
	        \hat{\theta}_{ridge} = (\mathbb{X}^T\mathbb{X} + n\lambda I)^{-1} 
	    \end{equation*}
	    I: Identity matrix
	    Unlike OLS, there always exists a unique optimal parameter vector for Ridge Regression.\\
        This is important, you should remember it!

	\end{frame}
	
	
	\begin{frame}{LASSO  Regression}
	    “LASSO Regression” is a term for the following specific combination of model, loss, and regularization:
	    \begin{itemize}
	        \item Model: $\mathbb{\hat{Y}} = \mathbb{X}\hat{\theta}$
	        \item Loss: Squared loss
	        \item Regularization: L1 regularization
	    \end{itemize}
	    \bigskip
	    The objective function we minimize for LASSO  Regression is average squared loss, plus an added penalty:
	    \begin{equation*}
	        \hat{\theta}_{LASSO} = \underset{\theta}{\text{arg min}}\frac{1}{n}||\mathbb{Y} - \mathbb{X}\theta||_2^2 + \lambda \sum\limits_{j=1}^d|\theta_i|
	    \end{equation*}
	\end{frame}
	
	
	
	\begin{frame}{LASSO  Regression}
	    We can also express this objective slightly differently:
	    \begin{equation*}
	        \hat{\theta}_{LASSO} = \underset{\theta}{\text{arg min}}\frac{1}{n}||\mathbb{Y} - \mathbb{X}\theta||_2^2 + \lambda \sum\limits_{j=1}^d|\theta_i|
	    \end{equation*}
	    
	    \begin{equation*}
	        \hat{\theta}_{LASSO} = \underset{\theta}{\text{arg min}}\frac{1}{n}||\mathbb{Y} - \mathbb{X}\theta||_2^2 + \lambda ||\theta ||_1
	    \end{equation*}
	    \bigskip
	   \\Unfortunately, there is no closed-form solution for the optimal parameter vector for LASSO. We must use numerical methods (like gradient descent).

	\end{frame}
	
	
	\begin{frame}{Summary of Regression Methods}
	   %\begin{array}{|l|l|l|l|l|l|}
	   %     \hline
	   %    Name & Model & Loss & Reg. & Objective & Solution \\
	   %    \hline 
	   %     OLS & \mathbb{\hat{Y} = \mathbb{X}\hat{\theta}} &  \text{Squared loss} & \text{None} & \frac{1}{n}||\mathbb{Y} - \mathbb{X}\hat{\theta}||^2_2 & \hat{\theta}_{OLS} = (\mathbb{X}^T\mathbb{X})^{-1}\mathbb{X}^T\mathbb{Y}\\
	   %     \hline
	   %     \text{Ridge Regression} & \mathbb{\hat{Y} = \mathbb{X}\hat{\theta}} & \text{Squared loss} & \text{L2} & \frac{1}{n}||\mathbb{Y} - \mathbb{X}\theta||_2^2 + \lambda \sum\limits_{j=1}^d\theta_i^2 & \hat{\theta}_{\text{ridge}} = (\mathbb{X}^T\mathbb{X} + n\lambda I)^{-1}\mathbb{X}^T\mathbb{Y}\\
	   %     \hline
    %         \text{LASSO}& \mathbb{\hat{Y} = \mathbb{X}\hat{\theta}} & \text{Squared loss}& \text{L1} & \frac{1}{n}||\mathbb{Y} - \mathbb{X}\theta||_2^2 + \lambda \sum\limits_{j=1}^d|\theta_i| & \text{no closed form}\\
    %         \hline
	   %\end{array}
	   
	   \begin{figure}
	       \centering
	       \includegraphics[scale=.4]{Bild19}
	   \end{figure}
	\end{frame}
	
	
	\begin{frame}{Fitting vs. Evaluating}
	While we may use a regularized objective function to determine our model’s parameters, we still look at (root) mean squared error to evaluate our model’s performance.
    
    \begin{equation*}
        \hat{\theta}_{ridge} = \underset{\theta}{\text{arg min}}\frac{1}{n}||\mathbb{Y} - \mathbb{X}\theta||_2^2 + \lambda \sum\limits_{j=1}^d\theta_i^2 
    \end{equation*}
    The regularization penalty is there for the purposes of model fitting only.
    \begin{equation*}
        RMSE = \sqrt{\frac{1}{n}\sum\limits_{i=1}^n(y_i - \mathbb{X}_i^T\hat{\theta}_{ridge})^2} = \sqrt{\frac{1}{n}||\mathbb{Y} - \mathbb{X}\hat{\theta}_{ridge}||_2^2}
    \end{equation*}
	\end{frame}
	
	
	\begin{frame}{Hyperparameters vs. Parameters}
        \begin{itemize}
            \item Parameters are facts about the world that we want to estimate
            \begin{itemize}
                \item Commonly denoted by p, $\theta, \theta_i$
            \end{itemize}
            \item Statistics are the estimators of the parameters, based on our data
            \begin{itemize}
                \item Commonly denoted by $\hat{p}, \hat{\theta}, \hat{\theta}_i$
            \end{itemize}
        \end{itemize}
        \bigskip
        Hyperparameters are design choices we make in our modeling process that affect our model, but do not directly come from the data

        \begin{itemize}
            \item examples: regularization hyperparameter, degree of polynomial
            \item Commonly denoted by $\lambda, \alpha, C$
        \end{itemize}
	\end{frame}
\end{document}