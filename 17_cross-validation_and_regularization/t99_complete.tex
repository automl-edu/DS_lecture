\documentclass[aspectratio=169]{../latex_main/tntbeamer}  % you can pass all options of the beamer class, e.g., 'handout' or 'aspectratio=43'
\input{../latex_main/preamble}

\title[Introduction]{DS: Cross-Validation and Regularization}
\subtitle{Segment Roadmap}

\graphicspath{ {./figure/} }
%\institute{}


\begin{document}
	
	\maketitle
	\begin{frame}{Segment Roadmap}
	    Sections 17.1 through 17.4 discuss train-test splits and cross-validation
	    \begin{itemize}
	        \item 17.1: why we need to split our data into train and test, and how cross-validation works
	        \item 17.2/17.3: creating train-test split and evaluating models fit on training data using testing data
	        \item 17.4: implementing cross-validation and using it to test different models.
	    \end{itemize}
	    Sections 17.5 and 17.6 discuss regularization.
	    \begin{itemize}
	        \item 17.5: why we need to regularize and how penalties on the norm of our parameter vector accomplish this goal
	        \item 17.6: explicitly lists the optimal model parameters when using L1 or L2 penalty on our linear model
	    \end{itemize}
	    [OPTIONAL] 17.7 is a great high-level overview of both the bias-variance tradeoff and regularization, created by GSI Paul Shao in Spring 2020.
	\end{frame}
	
	
		\begin{frame}{Training Error vs Test Error}
	    \includegraphics[scale=.4]{Bild1}
	\end{frame}
	
	
	\begin{frame}{Training Error vs Test Error}
	    \includegraphics[scale=.4]{Bild2}
	\end{frame}
	
	
	\begin{frame}{Generalization: The Train-Test Split}
	    \begin{columns}
	        \begin{column}{.5\textwidth}
	               \begin{itemize}
	                   \item Training Data: used to fit model
	                   \item Test Data: check generalization error
	                   \item How to split? 
	                   \begin{itemize}
	                       \item Randomly, Temporally, Geo…
	                       \item Depends on application (usually randomly)
	                   \end{itemize}
	                   \item What size? (90\%-10\%)
	                   \begin{itemize}
	                       \item Larger training set – more complex models
	                       \item Larger test set – better estimate of generalization error 
	                       \item Typically between 75\%-25\% and 90\%-10\%
	                   \end{itemize}
	               \end{itemize} 
	        \end{column}
	        
	        
	       \begin{column}{.5\textwidth}
	                \begin{figure}
	                    \includegraphics[scale=.5]{Bild3}
	                \end{figure}
	        \end{column}
	    \end{columns}
	    \bigskip
	    You can only use the test dataset once after deciding on the model.

	\end{frame}
	
	\begin{frame}{Generalization: Validation Split}
	    \includegraphics[scale=.43]{Bild4}\\
	    Cross validation simulates multiple train test-splits within the training data.

	\end{frame}
	
	\begin{frame}[c]{Recipe for Successful Generalization}
	    \begin{itemize}
	        \item Split your data into training and test sets (90\%, 10\%)
	        \item Use only the training data when designing, training, and tuning the model
	        \begin{itemize}
	            \item Use cross validation to test generalization during this phase
	            \item Do not look at the test data!
	        \end{itemize}
	        \item Commit to your final model and train once more using only the training data.
	        \item Test the final model using the test data. 
	        \item Train on all available data and ship it!
	    \end{itemize}
	\end{frame}
	
	
		\begin{frame}{Basic Idea}
	    \begin{equation*}
	        \hat{\theta} = \underset{\theta}{arg min} \frac{1}{n}\sum\limits_{i=1}^n\text{Loss}(y_i,f_\theta(x_i))
	    \end{equation*}
	    \underline{such that:}
	    \begin{equation*}
	        f_\theta \text{ does not 'overfit'}
	    \end{equation*}
	    Can we make this more formal?
	\end{frame}
	
	
	\begin{frame}[c]{Basic Idea}
	    \begin{equation*}
	        \hat{\theta} = \underset{\theta}{arg min} \frac{1}{n}\sum\limits_{i=1}^n\text{Loss}(y_i,f_\theta(x_i))
	    \end{equation*}
	    \underline{such that:}
	    \begin{equation*}
	         \text{Complexity}(f_\theta) \leq \beta
	    \end{equation*}
	    Complexity: How do we define this?\\
	    $\beta$:  Regularization Hyperparameter

	\end{frame}
	
	
	\begin{frame}[c]{Idealized Notion of Complexity}
	   \begin{equation*}
	         \text{Complexity}(f_\theta) \leq \beta
	    \end{equation*}
        \begin{itemize}
            \item Focus on complexity of linear models:
            \begin{itemize}
                \item Number and kinds of features
            \end{itemize}
            \item Ideal definition:
            \begin{equation*}
                \text{Complexity}(f_\theta) = \sum\limits_{j=1}^d\mathbb{I}[\theta_j \neq 0] 
            \end{equation*}
            \item Why?
        \end{itemize}
	\end{frame}
	
	
	\begin{frame}[c]{Ideal “Regularization”}
	Find the best value of $\theta$ which uses fewer than $\beta$ features.
	  \begin{equation*}
	        \hat{\theta} = \underset{\theta}{arg min} \frac{1}{n}\sum\limits_{i=1}^n\text{Loss}(y_i,f_\theta(x_i))
	    \end{equation*}
	    \underline{such that}
	    \begin{equation*}
                \text{Complexity}(f_\theta) = \sum\limits_{j=1}^d\mathbb{I}[\theta_j \neq 0] \leq \beta
        \end{equation*}
        Combinatorial search problem – NP-hard to solve in general.

	\end{frame}
	
	
	\begin{frame}{Norm Balls}
	    \includegraphics[scale=.35]{Bild5}
	\end{frame}
	
	\begin{frame}{Norm Balls}
	    \includegraphics[scale=.35]{Bild6}
	\end{frame}
	
	\begin{frame}{Norm Balls}
	    \includegraphics[scale=.35]{Bild7}
	\end{frame}
	
	\begin{frame}{Norm Balls}
	    \includegraphics[scale=.35]{Bild8}
	\end{frame}
	
	\begin{frame}{Norm Balls}
	    \includegraphics[scale=.35]{Bild9}
	\end{frame}
	
	
	\begin{frame}{Norm Balls}
	    \includegraphics[scale=.35]{Bild10}
	\end{frame}
	
	\begin{frame}{Norm Balls}
	    \includegraphics[scale=.35]{Bild11}
	\end{frame}
	
	\begin{frame}{Norm Balls}
	    \includegraphics[scale=.35]{Bild12}
	\end{frame}
	
	\begin{frame}{Norm Balls}
	    \includegraphics[scale=.35]{Bild13}
	\end{frame}
	
	
	\begin{frame}{Norm Balls}
	    \includegraphics[scale=.35]{Bild14}
	\end{frame}
	
	
	\begin{frame}{Norm Balls}
	    \includegraphics[scale=.35]{Bild15}
	\end{frame}
	
	
	\begin{frame}{Norm Balls}
	    \includegraphics[scale=.35]{Bild16}
	\end{frame}
	
	\begin{frame}{.}
	    \includegraphics[scale=.4]{Bild17}
	\end{frame}
	
	
	\begin{frame}{Generic Regularization (Constrained)}
	    Defining 
	    \begin{align*}
	        &\text{Complexity}(f_\theta) = R(\theta)\\
	        &\hat{\theta} = \underset{\theta}{arg min} \frac{1}{n}\sum\limits_{i=1}^n\text{Loss}(y_i,f_\theta(x_i))\\
	        &\underline{\text{such that: }} R(\theta) \leq \beta
	    \end{align*}
	    There is an equivalent unconstrained formulation (obtained by Lagrangian duality)

	\end{frame}
	
	
	\begin{frame}{Generic Regularization (Unconstrained)}
	    Defining 
	    \begin{align*}
	        &\text{Complexity}(f_\theta) = R(\theta)
	    \end{align*}
	    \begin{equation*}
	        \hat{\theta} = \underset{\theta}{\text{arg min}}\left[ \left(\frac{1}{n}\sum\limits_{i=1}^n\text{Loss}(y_i,f_\theta(x_i)) \right) + \lambda R(\theta)\right]
	    \end{equation*}
        $\lambda$: Regularization Hyperparameter

	\end{frame}
	
	
	\begin{frame}{Standardization and the Intercept Term}
	    \begin{itemize}
	        \item Height = $\theta_1$ age\_in\_seconds + $\theta_2$ weight\_in\_tons
	    \end{itemize}
	    $\theta_1$: small\\
	    $\theta_2$: large
	    \begin{columns}
	        \begin{column}{.5\textwidth}
	                \begin{itemize}
	                    \item Regularization penalized dimensions equally
	                    \item Standardization
	                    \begin{itemize}
	                        \item Ensure that each dimensions has the same scale
	                        \item centered around zero
	                    \end{itemize}
	                    \item Intercept Terms
	                    \begin{itemize}
	                        \item Typically don’t regularize intercept term 
	                    \end{itemize}
	                \end{itemize}
	        \end{column}
	        
	        \begin{column}{.5\textwidth}
	                \\ 
	                Standardization\\
	                For each dimension k:
	                \begin{equation*}
	                    Z_k = \frac{x_k - \mu_k}{\sigma_k}
	                \end{equation*}
	        \end{column}
	    \end{columns}
	\end{frame}
	
		\begin{frame}{Ridge Regression}
	    “Ridge Regression” is a term for the following specific combination of model, loss, and regularization:
	    \begin{itemize}
	        \item Model: $\mathbb{\hat{Y}} = \mathbb{X}\hat{\theta}$
	        \item Loss: Squared loss
	        \item Regularization: L2 regularization
	    \end{itemize}
	    The objective function we minimize for Ridge Regression is average squared loss, plus an added penalty:
	    \begin{equation*}
	        \hat{\theta}_{ridge} = \underset{\theta}{\text{arg min}}\frac{1}{n}||\mathbb{Y} - \mathbb{X}\theta||_2^2 + \lambda \sum\limits_{j=1}^d\theta_i^2 
	    \end{equation*}
	\end{frame}
	
	
	\begin{frame}{Ridge Regression}
	    We can also express this objective slightly differently:
	    \begin{equation*}
	        \hat{\theta}_{ridge} = \underset{\theta}{\text{arg min}}\frac{1}{n}||\mathbb{Y} - \mathbb{X}\theta||_2^2 + \lambda \sum\limits_{j=1}^d\theta_i^2 
	    \end{equation*}
	    
	    \begin{equation*}
	        \hat{\theta}_{ridge} = \underset{\theta}{\text{arg min}}\frac{1}{n}||\mathbb{Y} - \mathbb{X}\theta||_2^2 + \lambda ||\theta ||_2^2
	    \end{equation*}
	    $||\theta ||$: L2 norm of $\theta$ (hence, L2 regularization)\\
	    \bigskip
	    The latter representation ignores the fact that we typically don’t regularize the intercept term
	\end{frame}
	
	
	\begin{frame}[c]{Ridge Regression}
	    Ridge Regression has a closed form solution, conveniently:
	    \begin{equation*}
	        \hat{\theta}_{ridge} = (\mathbb{X}^T\mathbb{X} + n\lambda I)^{-1} 
	    \end{equation*}
	    I: Identity matrix
	    Unlike OLS, there always exists a unique optimal parameter vector for Ridge Regression.\\
        This is important, you should remember it!

	\end{frame}
	
	
	\begin{frame}{LASSO  Regression}
	    “LASSO Regression” is a term for the following specific combination of model, loss, and regularization:
	    \begin{itemize}
	        \item Model: $\mathbb{\hat{Y}} = \mathbb{X}\hat{\theta}$
	        \item Loss: Squared loss
	        \item Regularization: L1 regularization
	    \end{itemize}\\
	    \bigskip
	    The objective function we minimize for LASSO  Regression is average squared loss, plus an added penalty:
	    \begin{equation*}
	        \hat{\theta}_{LASSO} = \underset{\theta}{\text{arg min}}\frac{1}{n}||\mathbb{Y} - \mathbb{X}\theta||_2^2 + \lambda \sum\limimts_{j=1}^d|\theta_i|
	    \end{equation*}
	\end{frame}
	
	
	
	\begin{frame}{LASSO  Regression}
	    We can also express this objective slightly differently:
	    \begin{equation*}
	        \hat{\theta}_{LASSO} = \underset{\theta}{\text{arg min}}\frac{1}{n}||\mathbb{Y} - \mathbb{X}\theta||_2^2 + \lambda \sum\limits_{j=1}^d|\theta_i|
	    \end{equation*}
	    
	    \begin{equation*}
	        \hat{\theta}_{LASSO} = \underset{\theta}{\text{arg min}}\frac{1}{n}||\mathbb{Y} - \mathbb{X}\theta||_2^2 + \lambda ||\theta ||_1
	    \end{equation*}
	    \bigskip
	   \\Unfortunately, there is no closed-form solution for the optimal parameter vector for LASSO. We must use numerical methods (like gradient descent).

	\end{frame}
	
	
	\begin{frame}{Summary of Regression Methods}
	   %\begin{array}{|l|l|l|l|l|l|}
	   %     \hline
	   %    Name & Model & Loss & Reg. & Objective & Solution \\
	   %    \hline 
	   %     OLS & \mathbb{\hat{Y} = \mathbb{X}\hat{\theta}} &  \text{Squared loss} & \text{None} & \frac{1}{n}||\mathbb{Y} - \mathbb{X}\hat{\theta}||^2_2 & \hat{\theta}_{OLS} = (\mathbb{X}^T\mathbb{X})^{-1}\mathbb{X}^T\mathbb{Y}\\
	   %     \hline
	   %     \text{Ridge Regression} & \mathbb{\hat{Y} = \mathbb{X}\hat{\theta}} & \text{Squared loss} & \text{L2} & \frac{1}{n}||\mathbb{Y} - \mathbb{X}\theta||_2^2 + \lambda \sum\limits_{j=1}^d\theta_i^2 & \hat{\theta}_{\text{ridge}} = (\mathbb{X}^T\mathbb{X} + n\lambda I)^{-1}\mathbb{X}^T\mathbb{Y}\\
	   %     \hline
    %         \text{LASSO}& \mathbb{\hat{Y} = \mathbb{X}\hat{\theta}} & \text{Squared loss}& \text{L1} & \frac{1}{n}||\mathbb{Y} - \mathbb{X}\theta||_2^2 + \lambda \sum\limits_{j=1}^d|\theta_i| & \text{no closed form}\\
    %         \hline
	   %\end{array}
	   
	   \begin{figure}
	       \centering
	       \includegraphics[scale=.4]{Bild19}
	   \end{figure}
	\end{frame}
	
	
	\begin{frame}{Fitting vs. Evaluating}
	While we may use a regularized objective function to determine our model’s parameters, we still look at (root) mean squared error to evaluate our model’s performance.
    
    \begin{equation*}
        \hat{\theta}_{ridge} = \underset{\theta}{\text{arg min}}\frac{1}{n}||\mathbb{Y} - \mathbb{X}\theta||_2^2 + \lambda \sum\limits_{j=1}^d\theta_i^2 
    \end{equation*}
    The regularization penalty is there for the purposes of model fitting only.
    \begin{equation*}
        RMSE = \sqrt{\frac{1}{n}\sum\limits_{i=1}^n(y_i - \mathbb{X}_i^T\hat{\theta}_{ridge})^2} = \sqrt{\frac{1}{n}||\mathbb{Y} - \mathbb{X}\hat{\theta}_{ridge}||_2^2}
    \end{equation*}
	\end{frame}
	
	
	\begin{frame}{Hyperparameters vs. Parameters}
        \begin{itemize}
            \item Parameters are facts about the world that we want to estimate
            \begin{itemize}
                \item Commonly denoted by p, $\theta, \theta_i$
            \end{itemize}
            \item Statistics are the estimators of the parameters, based on our data
            \begin{itemize}
                \item Commonly denoted by $\hat{p}, \hat{\theta}, \hat{\theta}_i$
            \end{itemize}
        \end{itemize}\\
        \bigskip
        Hyperparameters are design choices we make in our modeling process that affect our model, but do not directly come from the data

        \begin{itemize}
            \item examples: regularization hyperparameter, degree of polynomial
            \item Commonly denoted by $\lambda, \alpha, C$
        \end{itemize}
	\end{frame}
\end{document}