\documentclass[aspectratio=169]{../latex_main/tntbeamer}  % you can pass all options of the beamer class, e.g., 'handout' or 'aspectratio=43'
\input{../latex_main/preamble}

\title[Gradient-Boosting Trees]{DS: Decision Trees}
\subtitle{Gradient-Boosting Trees}

\graphicspath{ {./figure_tree/} }
%\institute{}


\begin{document}
	
	\maketitle
	\begin{frame}[c]{Gradient-Boosting Trees}

    \begin{itemize}
        \item The most popular model in Kaggle Competitions\footnote{\url{https://www.kaggle.com/}} for tabular data is XGBoost
        \begin{itemize}
            \item XGBoost = Extreme Gradient Boosted Trees
        \end{itemize}
        \smallskip
        \item Main Idea:
        \begin{itemize}
            \item Random Forest build an ensemble of independent decision trees
            \item Instead of independent trees, try to improve the first trees with the following trees
            \item Each additional tree should directly contribute to improve the ensemble (not only by chance!)
        \end{itemize}
    \end{itemize}

	\end{frame}
	
	\begin{frame}[c]{Correcting Predictions}

    \vspace{-1em}
    \begin{itemize}
        \item Let's assume that we fit $m$ trees $\hat{F}_m$ already
        \item All of these together provide a prediction for a given input:
        $$ \hat{F}_m(\bm{x}_i) = \hat{y}_i $$
        \smallskip
        \item The model will now be off from the true label to a certain degree
        $$e_i = y_i - \hat{F}_m(\bm{x}_i)$$
        \item Remember: residuals!
        \smallskip
        \item Can we add a model that correct our ensemble $\hat{F}_m$ in the directions of the residuals by adding another model $\hat{f}_{m+1}$?
        $$ \hat{f}_{m+1} :=  y_i - \hat{F}_m(\bm{x}_i)$$ 
        \item if $ \hat{f}_{m+1}$ could perfectly learn the residuals, $\hat{f}_{m+1} +  \hat{F}_m(\bm{x}_i)$ would be a perfect model
    \end{itemize}

	\end{frame}
	
	\begin{frame}[c]{Learning how to boost}

    \vspace{-1em}
    \begin{itemize}
        \item Coming back to the idea of ensembles $F$ (with $M$ base models) by weighting $\gamma$ each model:
        
        $$ \hat{F}(\bm{x}) = \sum_{m=1}^{M} \gamma_m \hat{f}_m(\bm{x}) + c $$
        \item Let's start with fitting a first constant base model on all training data
        $$F_0 = \argmin_{\gamma} \sum \mathcal{L}(y_i, \gamma) $$
        \item Extend the ensemble by a base model correcting for the residuals -- recursive definition!
        
        $$ F_m (\bm{x}) = F_{m-1}(\bm{x}) + \argmin_{h_m} \left[ \sum_i \mathcal{L}(y_i, F_{m-1}(\bm{x}_i) + f_m(\bm{x}_i) \right] $$
    \end{itemize}

	\end{frame}
	
	\begin{frame}[c]{Learning how to boost (cont'd)}

    \vspace{-1em}
    \begin{itemize}
        
        $$ F_m (\bm{x}) = F_{m-1}(\bm{x}) + \argmin_{h_m} \left[ \sum_i \mathcal{L}(y_i, F_{m-1}(\bm{x}_i) + f_m(\bm{x}_i) \right] $$
        \item[$\leadsto$] that would be an infeasible optimization problem
        \item[$\leadsto$] take a step in the steepest direction (steepest descent) of this minimization problem:
        $$ F_m(x) = F_{m-1}(\bm{x}) - \gamma \sum_i \nabla_{F_{m-1}} \mathcal{L}(y_i, F_{m-1}(\bm{x_i})) $$
        \item where $\gamma$ is the step length
    \end{itemize}

	\end{frame}
	
	
\end{document}