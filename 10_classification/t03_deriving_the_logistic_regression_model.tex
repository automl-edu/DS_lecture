\documentclass[aspectratio=169]{../latex_main/tntbeamer}  % you can pass all options of the beamer class, e.g., 'handout' or 'aspectratio=43'
\input{../latex_main/preamble}

\title[LR: Derivation]{DS: Classification}
\subtitle{Deriving Logistic Regression}

\graphicspath{ {./figure/} }
%\institute{}


\begin{document}
	
	\maketitle
	\begin{frame}{Example dataset}
	    \begin{columns}
	        \begin{column}{.4\textwidth}
	                In this lecture, we will primarily use data from the 2017-18 NBA season.\\  \bigskip
	                Goal: Predict whether or not a team will win, given their FG\_PCT\_DIFF.
	                \begin{itemize}
	                    \item This is the difference in field goal percentage between the two teams.
	                    \item Positive FG\_PCT\_DIFF: team made more shots than the opposing team.
	                \end{itemize}
	        \end{column}
	        
	        \begin{column}{.4\textwidth}

	                    \includegraphics[scale=.35]{Bild3}
	                    
	        \end{column}
	    \end{columns}
	\end{frame}
	
	
	\begin{frame}{Why not use Ordinary Least Squares?}
	    \begin{columns}
	        \begin{column}{.4\textwidth}
	                We already have a model that can predict any quantitative response. Why not use it here?
	                \begin{itemize}
	                    \item The output can be outside of the range [0, 1]. What does a predicted WON value of -2 mean?
	                    \item Very sensitive to outliers.
	                    \item Many other statistical reasons.
	                    \begin{itemize}
	                        \item Not the point of our class.
	                    \end{itemize}
	                \end{itemize}
	        \end{column}
	        
	        \begin{column}{.4\textwidth}

	                    \includegraphics[scale=.5]{Bild4}
	        \end{column}
	    \end{columns}
	\end{frame}
	
	
		\begin{frame}{Graph of averages}
	    \begin{columns}
	        \begin{column}{.5\textwidth}
	                When defining the simple linear regression model, we binned the x-axis, and took the average y-value for each bin, and tried to model that.\\
	                \bigskip
	                Doing so here yields a curve that resembles an s.
	                \begin{itemize}
	                    \item Since our true y is either 0 or 1, this curve models the probability that WON = 1, given FG\_PCT\_DIFF.
	                    \begin{itemize}
	                        \item WON = 1 means “belong to class 1”.
	                    \end{itemize}
	                    \item Our goal is to model this red curve as best as possible.
	                \end{itemize}
	        \end{column}
	        
	        \begin{column}{.4\textwidth}

	                    \includegraphics[scale=.5]{Bild5}
	        \end{column}
	    \end{columns}
	\end{frame}
	
	
	\begin{frame}{Log-odds of probability is roughly linear}
	    The log-odds of the probability of belonging to Class 1 ($p = 1$) was linear. This is the assumption that logistic regression is based on. \\
	    \begin{equation*}
	        \text{odds(p)} = \frac{p}{1 - p}\hspace{1cm} \text{log-odds(p)} = \text{log}\left(\frac{p}{1 - p}\right)
	    \end{equation*}
	    
	    For now, let’s denote $t$ as our linear function (since log-odds is linear). Solving for p:
	    \begin{align*}
	        t &= \text{log}\left(\frac{p}{1 - p}\right)\rightarrow \text{With logistic regression, we are always referring to log base e (“ln”).
} \\
	        e^t &= \frac{p}{1 - p}\\
	        e^t - pe^t &= p\\
	        p &= \frac{e^t}{1 + e^t} = \frac{1}{1 + e^{-t}}
	    \end{align*}
	\end{frame}
	
	\begin{frame}{Log-odds of probability is roughly linear}
	    The log-odds of the probability of belonging to class 1 was linear. This is the assumption that logistic regression is based on. \\
	    \begin{equation*}
	        \text{odds(p)} = \frac{p}{1 - p}\hspace{1cm} \text{log-odds(p)} = \text{log}\left(\frac{p}{1 - p}\right)
	    \end{equation*}
	    
	    For now, let’s let t denote our linear function (since log-odds is linear). Solving for p:
	    \begin{align*}
	        t &= \text{log}\left(\frac{p}{1 - p}\right)\\
	        e^t &= \frac{p}{1 - p}\\
	        e^t - pe^t &= p\\
	    \end{align*}
	\end{frame}
	
	
	
	\begin{frame}{Arriving at the logistic regression model}
	    We know how to model linear functions quite well.\\
	    \begin{itemize}
	        \item We can substitute          $t = \bm{x}^\intercal\bm{\theta} $           , since t was just a placeholder.
	    \end{itemize}
	    \begin{equation*}
	        p = \frac{1}{1 + e^{-t}} = \sigma (t)
	    \end{equation*}
	    p represents the probability of belonging to class 1.
	    \begin{itemize}
	        \item We are modeling          $P(Y=1|\bm{x})$.
	    \end{itemize}
	    Putting this all together:
	    \begin{equation*}
	        P(Y=1|\bm{x}) = \frac{1}{1 + e^{-\bm{x}^\intercal\bm{\theta}}} = \sigma(\bm{x}^\intercal\bm{\theta})
	    \end{equation*}
	    Looks just like the linear regression model, with a $\sigma()$ wrapped around it.
We call logistic regression a generalized linear model, since it is a non-linear transformation of a linear model.

	\end{frame}
	
	\begin{frame}{Arriving at the logistic regression model}
	    \includegraphics[scale=.4]{Bild6}
	\end{frame}
	
	\begin{frame}{Shape of the logistic function}
	   
	   \begin{columns}
	    
	   \begin{column}{.4\textwidth}
	   Consider the plot of       $\sigma (\bm{x}^\intercal\bm{\theta_1})$      , for several different values of $\bm\theta_1$. 
	   \begin{itemize}
	       \item If $\bm\theta_1$ is positive, the curve increases to the right.
	       \item The further  $\bm\theta_1$ is from 0, the steeper the curve.
	   \end{itemize}
	   \end{column}
	    
	    \begin{column}{.6\textwidth}

	            \centering	            \includegraphics[width=\textwidth]{Bild10}
	            
	    \end{column}
	    
	    \end{columns}
	\end{frame}
	
\end{document}