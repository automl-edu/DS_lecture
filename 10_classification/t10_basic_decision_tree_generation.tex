\documentclass[aspectratio=169]{../latex_main/tntbeamer}  % you can pass all options of the beamer class, e.g., 'handout' or 'aspectratio=43'
\input{../latex_main/preamble}

\title[Introduction]{DS: Decision Trees}
\subtitle{Basic Decision Tree Generation}

\graphicspath{ {./figure_tree/} }
%\institute{}


\begin{document}
	
	\maketitle
	\begin{frame}{Basic Decision Tree Generation}
	    \begin{figure}
	        \centering
	        \includegraphics[scale=.3]{Bild40}
	    \end{figure}
	\end{frame}
	
	\begin{frame}{Decision Tree Generation}
	    Let’s first discuss how decision trees are created from data\\
	    \bigskip
	    Traditional decision tree generation algorithm: 
	    \begin{itemize}
	        \item All of the data starts in the root node
	        \item Repeat until every node is either pure or unsplittable:
	        \begin{itemize}
	            \item Pick the best feature x and best split value $\beta$, e.g. x = petal\_length, $\beta$ = 2
	            \item Split data into two nodes, one where x < $\beta$, and one where x ≥ $\beta$
	        \end{itemize}
	    \end{itemize}
	    
	    
Notes: A node that has only one type is called a “pure” node. A node that has duplicate data that cannot be split is called “unsplittable”.

	\end{frame}
	
	
	\begin{frame}{Defining a Best Feature}
	    Question: Which feature and split value is best?
	    \begin{itemize}
	        \item  Equivalently: Which horizontal or vertical line do we want to draw?
	    \end{itemize}
	    \begin{figure}
	        \centering
	        \includegraphics[scale=.4]{Bild41}
	    \end{figure}
	\end{frame}
	
	\begin{frame}{Defining a Best Feature}
	    Question: Which feature and split value is best?
	    \begin{itemize}
	        \item  Equivalently: Which horizontal or vertical line do we want to draw?
	    \end{itemize}
	    \begin{figure}
	        \centering
	        \includegraphics[scale=.4]{Bild42}
	    \end{figure}
	\end{frame}
	
	\begin{frame}{Defining a Best Feature}
	    Question: Which feature and split value is best?
	    \begin{itemize}
	        \item  Equivalently: Which horizontal or vertical line do we want to draw?
	    \end{itemize}
	    \begin{figure}
	        \centering
	        \includegraphics[scale=.4]{Bild43}
	    \end{figure}
	\end{frame}
	
	\begin{frame}{Defining a Best Feature}
	    Question: Which feature and split value is best?
	    \begin{itemize}
	        \item  Equivalently: Which horizontal or vertical line do we want to draw?
	    \end{itemize}
	    \begin{figure}
	        \centering
	        \includegraphics[scale=.4]{Bild44}
	    \end{figure}
	\end{frame}
	
	\begin{frame}{Defining a Best Feature}
	    Question: Which feature and split value is best?
	    \begin{itemize}
	        \item  Equivalently: Which horizontal or vertical line do we want to draw?
	    \end{itemize}
	    \begin{figure}
	        \centering
	        \includegraphics[scale=.4]{Bild45}
	    \end{figure}
	\end{frame}
	
	
	\begin{frame}{Defining a Best Feature}
	    Question: Which feature and split value is best?
	    \begin{itemize}
	        \item  Equivalently: Which horizontal or vertical line do we want to draw?
	    \end{itemize}
	    We need some sort of rigorous definition for a good split
	    \begin{figure}
	        \centering
	        \includegraphics[scale=.4]{Bild45}
	    \end{figure}
	\end{frame}
	
	
	\begin{frame}{Defining a Best Feature}
	    Question: Which feature and split value is best?
	    \begin{itemize}
	        \item  Equivalently: Which horizontal or vertical line do we want to draw?
	    \end{itemize}
	    We need some sort of rigorous definition for a good split
	    \begin{figure}
	        \centering
	        \includegraphics[scale=.4]{Bild46}
	    \end{figure}
	\end{frame}
	
	
	
	\begin{frame}{Node Entropy}
	
	    \vspace{-2em}
	    \begin{columns}
	        \begin{column}{.5\textwidth}
	        
	                Let $p_c$ the proportion of data points in a node with label C.\\
	                \bigskip
	                For example, for the node at the top of the decision tree, $p_0$ = 34/110 = 0.31,\\
                    $p_1$ = 36/110 = 0.33, and $p_2$ = 40/110 = 0.36\\
                    \bigskip
                    Define the entropy $S$ of a node as:
                    \begin{equation*}
                        S = -\sum\limits_c p_c\log_2p_c
                    \end{equation*}
                    For example, $S$ for the top node is:
                    $−0.31 \log_2{⁡0.31} − 0.33 \log_2{⁡0.33} − 0.36 \log_2{⁡0.36}    = 0.52 + 0.53 + 0.53 = 1.58$
                    
	        \end{column}
	        
	        \begin{column}{.5\textwidth}

	                    \centering
	                    \includegraphics[scale=.5]{Bild47}
	                    
	        \end{column}
	    \end{columns}
	\end{frame}
	
	
% 	\begin{frame}{Test Your Understanding}
% 	    What is $p_0$?
% 	    \begin{figure}
%             \centering
%             \includegraphics[scale=.5]{Bild47}
% 	   \end{figure}
% 	   What is the entropy of the node on the left with [31, 4, 1] in each class?
% 	   \begin{itemize}
% 	       \item Try writing out an expression, or try to compute an exact value
% 	   \end{itemize}
% 	   \begin{equation*}
%             S = -\sum\limits_c p_c\log_2p_c
%         \end{equation*}
% 	\end{frame}
	
	
% 	\begin{frame}{Test Your Understanding}
	
% 	    \vspace{-2em}
% 	    \begin{columns}
% 	        \begin{column}{.7\textwidth}
	            
% 	                What is the entropy of the node on the left with [31, 4, 1] in each class?
% 	                \begin{itemize}
% 	                    \item $p_0$ = 31/36 = 0.86, $p_1$ = 4/36 = 0.11, and $p_2$=1/36 = 0.028
% 	                    \item $S$ =  −0.86 log$_2$⁡0.86 
% 	                    \begin{itemize}
% 	                        \item 0.11 log$_2$⁡0.11 
% 	                        \item 0.028 log$_2$⁡0.028 = 0.68
% 	                    \end{itemize}
% 	                \end{itemize}
% 	                Define the entropy $S$ of a node as:
%                     \begin{equation*}
%                         S = -\sum\limits_c p_c\log_2p_c
%                     \end{equation*}
%                     Can think of entropy as how unpredictable a node is:
%                     \begin{itemize}
%                         \item Low entropy means more predictable 
%                         \item High entropy means more unpredictable
%                     \end{itemize}
                    
% 	        \end{column}
	        
% 	        \begin{column}{.3\textwidth}

% 	                    \centering
% 	                    \includegraphics[scale=.35]{Bild47}
	                
% 	        \end{column}
% 	    \end{columns}
% 	\end{frame}
	
	
	\begin{frame}{Exploring Entropy}
	    \begin{columns}
	        \begin{column}{.7\textwidth}
	                Observations about entropy:
	                \begin{itemize}
	                    \item A node where all data are part of the same class has zero entropy \\ $-1 \log_2 1 = 0$
	                    \item A node where data are evenly split between two classes has
	                    entropy 1 \\$−0.5 \log_2{ 0.5} − 0.5\log_2{ 0.5}= 1$
	                    \item A node where data are evenly split between 3 classes has entropy 1.58 \\$3 \cdot (−0.33 \log_2{ 0.33}) = 1.58$
	                    \item A node where data are evenly split into C classes has entropy $\log_2{C}$ \\ $C \cdot (−1/C \log_{2}{⁡ 1/C}) = −\log_{2}{⁡ 1/C} = \log_{2}{⁡C}$
	                \end{itemize}
	        \end{column}
	        
	        \begin{column}{.3\textwidth}
	                \begin{figure}
	                    \centering
	                    \includegraphics[scale=.35]{Bild47}
	                \end{figure}
	                \begin{equation*}
                          S = -\sum\limits_c p_c\log_2p_c
                    \end{equation*}
	                
	        \end{column}
	    \end{columns}
	\end{frame}
	
	
	\begin{frame}[c]{Weighted Entropy as a Loss Function}
	    We can use Weighted Entropy as a loss function in helping us decide which split to take\\
	    \bigskip
	    Suppose a given split results in two nodes X and Y with N1 and N2 total samples each. The loss of that split is given by:
	    \begin{equation*}
	        \mathcal{L}=\frac{N_1S(X) + N_2S(Y)}{N_1 + N_2}
	    \end{equation*}
	\end{frame}
	
	\begin{frame}{Defining a Best Feature}
	    Split choice \#1: width > 1.5. Compute entropy of child nodes:
	    \begin{itemize}
	        \item entropy([50, 46, 3]) = 1.16
	        \item entropy([4, 47]) = 0.4
	        \item Weighted loss: 99/150 × 1.16 + 51/150 × 0.4 = 0.9
	    \end{itemize}
	    
	    \begin{figure}
	        \centering
	        \includegraphics[scale=.4]{Bild48}
	    \end{figure}
	\end{frame}
	
	
	\begin{frame}{Defining a Best Feature}
	    Split choice \#2: length > 4. Compute entropy of child nodes:
	    \begin{itemize}
	        \item entropy([50, 9]) = 0.62
	        \item entropy([41, 50]) = 0.99
	        \item Weighted loss: 0.84: Better/smaller than split choice \#1!
	    \end{itemize}
	    
	    \begin{figure}
	        \centering
	        \includegraphics[scale=.4]{Bild49}
	    \end{figure}
	\end{frame}
	
	
	\begin{frame}{Defining a Best Feature}
	    Split choice \#3: width > 0.5. Compute entropy of child nodes:
	    \begin{itemize}
	        \item entropy([2, 50, 50]) = 1.12
	        \item entropy([48]) = 0
	        \item Weighted loss: 0.76: Lower than split choice \#2!

	    \end{itemize}
	    
	    \begin{figure}
	        \centering
	        \includegraphics[scale=.4]{Bild50}
	    \end{figure}
	\end{frame}
	
	
	\begin{frame}{Defining a Best Feature}
 	    Split choice \#4: width > 0.9. Compute entropy of child nodes:
	    \begin{itemize}
	        \item entropy([50, 50]) = 1
	        \item entropy([50]) = 0
	        \item Weighted loss: 0.66: Lower than split choice \#3!

	    \end{itemize}
	    
	    \begin{figure}
	        \centering
	        \includegraphics[scale=.4]{Bild51}
	    \end{figure}
	\end{frame}
	
	
	\begin{frame}{Decision Tree Generation}
	    Traditional decision tree generation algorithm: 
	    \begin{itemize}
	        \item All of the data starts in the root node
	        \item Repeat until every node is either pure or unsplittable:
	        \begin{itemize}
	            \item Pick the best feature $f_i$ and split value $\beta$ such that the loss of the resulting split is minimized, e.g. $f_i$ = petal\_width, $\beta$ = 0.8 has loss 0.66
	            \item Split data into two nodes, one where $x < \beta$ , and one where $x \geq \beta$ 
	        \end{itemize}
	    \end{itemize}
	    Notes: A node that has only one type is called a “pure” node. A node that has duplicate data that cannot be split is called “unsplittable” \\

	\end{frame}
\end{document}